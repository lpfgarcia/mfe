% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/landmarking.R
\name{mf.landmarking}
\alias{mf.landmarking}
\alias{mf.landmarking.default}
\alias{mf.landmarking.formula}
\title{Landmarking Meta-features}
\usage{
mf.landmarking(...)

\method{mf.landmarking}{default}(x, y, features = "all", summary = c("mean",
  "sd"), folds = 10, score = "accuracy", ...)

\method{mf.landmarking}{formula}(formula, data, features = "all",
  summary = c("mean", "sd"), folds = 10, score = "accuracy", ...)
}
\arguments{
\item{...}{Further arguments passed to the summarization functions.}

\item{x}{A data.frame contained only the input attributes.}

\item{y}{A factor response vector with one label for each row/component of x.}

\item{features}{A list of features names or \code{"all"} to include all them.}

\item{summary}{A list of summarization functions or empty for all values. See
\link{post.processing} method to more information. (Default: 
\code{c("mean", "sd")})}

\item{folds}{The number of k equal size subsamples in k-fold 
cross-validation.(Default: 10)}

\item{score}{The evaluation measure used to score the classification 
performance. \code{c("accuracy", "balanced.accuracy", "kappa")}. 
(Default: \code{"accuracy"}).}

\item{formula}{A formula to define the class column.}

\item{data}{A data.frame dataset contained the input attributes and class.
The details section describes the valid values for this group.}
}
\value{
A list named by the requested meta-features.
}
\description{
Landmarking measures are simple and fast algorithms, from which performance
can be extracted.
}
\details{
The following features are allowed for this method:
 \describe{
   \item{"dn"}{Decision node. Construct a single DT node model induced by the
   most informative attribute.}
   \item{"enn"}{Elite nearest neighbor. Use the most informative attributes 
   in the dataset using the varImportance to induce the 1-Nearest Neighbor. 
   With the subset of informative attributes is expected that the models 
   induced by 1-Nearest Neighbor should be noise tolerant.}
   \item{"ld"}{Linear discriminant. Apply the Linear Discriminant classifier 
   to construct a linear split (non parallel axis) in the data to establish
   the linear separability.}
   \item{"nb"}{Naibe Bayes. Evaluate the performance of the Naive Bayes
     classifier. It assumes that the attributes are independent and each
     example belongs to a certain class based on the Bayes probability.} 
   \item{"nn"}{One nearest neighbor. Evaluate the performance of the
     1-Nearest Neighbor classifier. It uses the euclidean distance of the
     nearest neighbor to determine how noisy is the data.}
   \item{"rn"}{Random node. Construct a single DT node model induced by a
   random attribute.}
   \item{"wn"}{Worst node. Construct a single DT node model induced by the
   worst informative attribute.}
 }
}
\examples{
## Extract all meta-features using formula
mf.landmarking(Species ~ ., iris)

## Extract some meta-features
mf.landmarking(iris[1:4], iris[5], c("dn", "rn", "wn"))

## Use another summarization function
mf.landmarking(Species ~ ., iris, summary=c("min", "median", "max"))

## Use 2 folds and balanced accuracy
mf.landmarking(Species ~ ., iris, folds=2, score="balanced.accuracy")
}
\references{
Pfahringer, B., Bensusan, H., &  Giraud-Carrier, C. G. (2000). Meta-Learning
 by Landmarking Various Learning Algorithms. In Proceedings of the 17th
 International Conference on Machine Learning (pp. 743-750)
}
\seealso{
Other meta-features: \code{\link{mf.discriminant}},
  \code{\link{mf.general}}, \code{\link{mf.infotheo}},
  \code{\link{mf.model.based}},
  \code{\link{mf.statistical}}
}
